{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbaH0CxRewN7"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIOprvzpewOB"
   },
   "source": [
    "In this notebook, you will review the singular value decomposition (SVD) of matrices and learn about some of its applications in data analysis.  This notebook is heavy on linear algebra, so you may need to go back and review various concepts along the way.  Pretty much any standard linear algebra text should suffice.  Some free, online options can be found [here](https://aimath.org/textbooks/approved-textbooks/).\n",
    "\n",
    "In this notebook, you'll also use the two included image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mpdoRjjewOB"
   },
   "source": [
    "# Rank-one matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOch5ttFewOC"
   },
   "source": [
    "A *rank-one matrix* is an $m\\times n$ matrix of the form $A=ab^T$, where $a$ is a nonzero vector in $\\mathbb{R}^m$ and $b$ is a nonzero vector in $\\mathbb{R}^n$.  As we'll see, the rank-one matrices play a special role as \"atoms\" in the singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2JBNYvDewOC"
   },
   "source": [
    "## Why they're called that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJardXiTewOD"
   },
   "source": [
    "As you might expect, such a matrix is called *rank-one*, because its rank is... one.  One way to see this is through the [rank-nullity theorem](https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem) of linear algebra.   (If you're rusty, you may want to review the concepts of rank, nullity, and the rank-nullity theorem.)\n",
    "\n",
    "The rank-nullity theorem says that for a general $m\\times n$ matrix $A$,\n",
    "\n",
    "$$\n",
    "\\text{rank}(A) + \\text{nullity}(A) = n.\n",
    "$$\n",
    "\n",
    "Let's take a look at the case $A=ab^T$ (with both $a$ and $b$ nonzero).  Note that\n",
    "\n",
    "$$\n",
    "Ab = (ab^T)b = a(b^Tb) = a\\|b\\|^2 = \\|b\\|^2a,\n",
    "$$\n",
    "\n",
    "which is a scaled version of the vector $a$.  This shows that the nonzero vector $a$ is in the range of $A$ and, therefore, that $\\text{rank}(A)\\ge 1$.  On the other hand, if $c$ is any vector that is orthogonal to $b$, then\n",
    "\n",
    "$$\n",
    "Ac = (ab^T)c = a(b^Tc) = a\\,0 = 0.\n",
    "$$\n",
    "\n",
    "The set of all vectors orthogonal to $b$ is an $(n-1)$-dimensional subspace of $\\mathbb{R}^n$ and so the nullity of $A$ is at least $n-1$.\n",
    "\n",
    "If we combine all of this together, we get\n",
    "\n",
    "$$\n",
    "1 \\le \\text{rank}(A) = n - \\text{nullity}(A) \\le n - (n-1) = 1,\n",
    "$$\n",
    "\n",
    "so $\\text{rank}(A)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVovhu5qewOE"
   },
   "source": [
    "## What they look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEpHMJhpewOE"
   },
   "source": [
    "Using the rules of matrix multiplication, if\n",
    "\n",
    "$$\n",
    "    a = [a_1\\,a_2\\,\\ldots\\,a_m]^T\\qquad\\text{and}\\qquad b=[b_1\\,b_2\\,\\ldots\\,b_n]^T,\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "    ab^T =\n",
    "    \\begin{bmatrix}\n",
    "        a_1\\\\a_2\\\\\\vdots\\\\a_m\n",
    "    \\end{bmatrix}\n",
    "    [b_1\\;b_2\\;\\cdots\\;b_n]\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        a_1b_1 & a_1b_2 & a_1b_3 & \\cdots & a_1b_n\\\\\n",
    "        a_2b_1 & a_2b_2 & a_2b_3 & \\cdots & a_2b_n\\\\\n",
    "        \\vdots & \\vdots & \\vdots &        & \\vdots\\\\\n",
    "        a_mb_1 & a_mb_2 & a_mb_3 & \\cdots & a_mb_n\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can also write the product in a couple of different helpful [block matrix](https://en.wikipedia.org/wiki/Block_matrix) forms:\n",
    "\n",
    "$$\n",
    "    ab^T = [ b_1a\\quad b_2a\\quad b_3a\\quad\\cdots\\quad b_na] = \n",
    "    \\begin{bmatrix}\n",
    "    a_1b^T\\\\a_2b^T\\\\a_3b^T\\\\\\vdots\\\\a_mb^T\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The first form shows that all columns of the matrix are parallel to a single vector, $a$.  The second form shows that all rows are parallel to a single vector, $b$.  Both forms also demonstrate why the rank of this matrix is 1: the dimension of the row and column spaces are both 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rob68JLbewOF"
   },
   "source": [
    "## Block matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzEfea_jewOG"
   },
   "source": [
    "Block matrix manipulations are an important and powerful tool in working with matrices, and are worth reviewing, especially since they will come up frequently in this class.  The following video explains the concept and shows how to use block matrix multiplication to understand operations like matrix-vector multiplication and matrix-matrix multiplication.\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://youtu.be/s7xw7pZ_vJk\">\n",
    "                <img src=\"https://img.youtube.com/vi/s7xw7pZ_vJk/hqdefault.jpg\"><br>\n",
    "                Block matrix manipulation\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__pjL8dlewOG"
   },
   "source": [
    "# The SVD in rank-one form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEdeuyykewOG"
   },
   "source": [
    "## The main theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypof6T5SewOH"
   },
   "source": [
    "One way to state the main theorem is the following.\n",
    "\n",
    "**Theorem (The SVD):** Let $A\\in\\mathbb{R}^{m\\times n}$ have rank $r=\\text{rank}(A)$.  Then there exists an orthonormal basis $\\{u_i\\}$ of $\\mathbb{R}^m$, an orthonormal basis $\\{v_i\\}$ of $\\mathbb{R}^n$, and positive numbers $\\sigma_1\\ge\\sigma_2\\ge\\cdots\\ge\\sigma_r>0$ such that\n",
    "\n",
    "$$\n",
    "    A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + \\cdots + \\sigma_ru_rv_r^T.\n",
    "$$\n",
    "\n",
    "**Notes**\n",
    "- The $\\sigma_i$ are called the *singular values* of $A$.\n",
    "- The $u_i$ are called *left singular vectors* of $A$.\n",
    "- The $v_i$ are called *right singular vectors* of $A$.\n",
    "- Often, the singular values will be extended by zeros so that there are $\\min\\{m,n\\}$ of them (see the \"thin\" and \"short\" forms later in this notebook): $\\sigma_1\\ge\\sigma_2\\ge\\cdots\\sigma_r>0=\\sigma_{r+1}=\\sigma_{r+2}=\\cdots=\\sigma_{\\min\\{m,n\\}}$.\n",
    "\n",
    "You can get a deeper understanding of why the SVD exists and its relationship to \"maximum stretch\" in the following video.  (NB: The vectors $u_i$ and $v_i$ are switched in the video.)\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://youtu.be/aXzR9ZRw4cc\">\n",
    "                <img src=\"https://img.youtube.com/vi/aXzR9ZRw4cc/hqdefault.jpg\"><br>\n",
    "                The SVD Algorithm\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QiD94MAewOH"
   },
   "source": [
    "## The $U\\Sigma V^T$ forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yba6fxcVewOH"
   },
   "source": [
    "The SVD of a matrix is often written in the form $A=U\\Sigma V^T$.  Unfortunately, different books define the various matrices in the product a little differently.  Here's an attempt to unify and demystify the different options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO8b6SvYewOI"
   },
   "source": [
    "### Compact form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4iDMaHsewOI"
   },
   "source": [
    "The most compact form of the SVD comes from making the following definitions:\n",
    "\n",
    "$$\n",
    "    U_r = [u_1\\,u_2\\,\\cdots\\,u_r], \\qquad V_r=[v_1\\,v_2\\,\\cdots\\,v_r], \\qquad \\Sigma_r=\\text{diag}(\\sigma_1,\\sigma_2,\\ldots,\\sigma_r).\n",
    "$$\n",
    "\n",
    "$U_r$ is the $m\\times r$ matrix whose columns are $u_1$ through $u_r$.  $V_r$ is the $n\\times r$ matrix whose columns are $v_1$ through $v_r$ and $\\Sigma_r$ is the $r\\times r$ diagonal matrix with diagonal entries $\\sigma_1$ through $\\sigma_r$.\n",
    "\n",
    "If you've practiced your block matrix operations, you'll see that\n",
    "\n",
    "$$\n",
    "    A = \\sum_{i=1}^r \\sigma_iu_iv_i^T = U_r\\Sigma_r V_r^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI7E3_1yewOI"
   },
   "source": [
    "### Full form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-pwHPwGewOI"
   },
   "source": [
    "Alternatively, we could define\n",
    "\n",
    "$$\n",
    "    U = [u_1\\,u_2\\,\\cdots\\,u_m]=[U_r\\;\\tilde{U}], \\qquad V=[v_1\\,v_2\\,\\cdots\\,v_n]=[V_r\\;\\tilde{V}], \\qquad \n",
    "    \\Sigma=\n",
    "    \\begin{bmatrix}\n",
    "    \\Sigma_r & 0_{r\\times (n-r)}\\\\\n",
    "    0_{(m-r)\\times r} & 0_{(m-r)\\times(n-r)}\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In this form, $\\tilde{U}$ is the matrix made up of the vectors $u_{r+1},u_{r+2},\\ldots u_{m}$ (the $u$ vectors missing from $U_r$), and similarly for $\\tilde{V}$.  $\\Sigma$ is defined as a block matrix made up of $\\Sigma_r$ and 3 zero blocks (with the sizes specified).  Notice that\n",
    "\n",
    "$$\n",
    "    U\\Sigma V^T = \n",
    "    [U_r\\;\\tilde{U}]\n",
    "    \\begin{bmatrix}\n",
    "    \\Sigma_r & 0_{r\\times (n-r)}\\\\\n",
    "    0_{(m-r)\\times r} & 0_{(m-r)\\times(n-r)}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    V_r^T\\\\\n",
    "    \\tilde{V}^T\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    [U_r\\;\\tilde{U}]\n",
    "    \\begin{bmatrix}\n",
    "    \\Sigma_rV_r^T\\\\\n",
    "    0_{(m-r)\\times n}\n",
    "    \\end{bmatrix}\n",
    "    = U_r\\Sigma_rV_r^T = A.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjt8ACCKewOJ"
   },
   "source": [
    "### Thin form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAZk6KhSewOJ"
   },
   "source": [
    "When $m\\ge n$, $A$ is called a *thin* matrix.  Often this is the first SVD form you'll come across in a book.  Typically, the singular values $\\sigma_i$ are extended so that $\\sigma_{r+1}=\\sigma_{r+2}=\\cdots=\\sigma_n=0$ and the $\\Sigma$ matrix will be the $n\\times n$ matrix $\\Sigma_n=\\text{diag}(\\sigma_1,\\sigma_2,\\ldots,\\sigma_n)$.  The matrix $U_n$ will be defined, as you might expect, as $U_n=[u_1\\,u_2\\,\\cdots\\,u_n]$ and, using the definition of $V$ above, the SVD is written as\n",
    "\n",
    "$$\n",
    "    A = U_n\\Sigma_n V^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7lzV3qJewOJ"
   },
   "source": [
    "### Short form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UutpRsTmewOJ"
   },
   "source": [
    "When $n\\ge m$, there is a corresponding *short* SVD form.\n",
    "\n",
    "$$\n",
    "    A = U\\Sigma_m V_m^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5evXn2syewOK"
   },
   "source": [
    "### Summary of SVD forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAFO-PSvewOK"
   },
   "source": [
    "Each of the above choices gives a representation of $A$ as a product $U\\Sigma V^T$.\n",
    "\n",
    "- $\\Sigma$ is always a diagonal (though not necessarily square) matrix.\n",
    "- $U$ and $V$ are always *orthogonal* matrices: $U^TU = I$ and $V^TV=I$.\n",
    "    - The dimension of the identity matrices $I$ are whatever makes sense from the product.\n",
    "    - Warning: In general, $UU^T\\ne I$ and $VV^T\\ne I$.  In fact, $UU^T=I$ if and only if $U$ is square, and similarly for $V$.\n",
    "- The shapes of the various matrices involved depend on the form of SVD used.  The matrix $A$ is assumed to be $m\\times n$ and to have rank $r$.\n",
    "    - Warning: Just as there is no standard SVD form, the different forms are also called by various names.  The form names in the table below are fairly common, but not universal.\n",
    "<table width=\"400px\">\n",
    "    <tr>\n",
    "        <th>form</th>\n",
    "        <th>$U$</th>\n",
    "        <th>$\\Sigma$</th>\n",
    "        <th>$V$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>compact</td>\n",
    "        <td>$m\\times r$</td>\n",
    "        <td>$r\\times r$</td>\n",
    "        <td>$n\\times r$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>full</td>\n",
    "        <td>$m\\times m$</td>\n",
    "        <td>$m\\times n$</td>\n",
    "        <td>$n\\times n$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>thin ($m\\ge n$)</td>\n",
    "        <td>$m\\times n$</td>\n",
    "        <td>$n\\times n$</td>\n",
    "        <td>$n\\times n$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>short ($m\\le n$)</td>\n",
    "        <td>$m\\times m$</td>\n",
    "        <td>$m\\times m$</td>\n",
    "        <td>$n\\times m$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "- The different forms are all useful for various reasons.  For example...\n",
    "    - Compact form uses the least amount of storage space.  $\\Sigma$ is a square, positive matrix.  The matrices $UU^T$ and $VV^T$ are projection matrices onto the range and nullspace of $A$.\n",
    "    - Full form gives a $\\Sigma$ with the same dimension as $A$.  $U$ and $V$ are invertible: $U^{-1}=U^T$ and $V^{-1}=V^T$.\n",
    "    - Thin form gives a square $\\Sigma$.  $V$ is invertible.  If $A$ has full rank, this is equivalent to the compact form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "127Zm6K-ewOK"
   },
   "source": [
    "### Moral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YpojXyCewOL"
   },
   "source": [
    "The moral to the story is that there are *many* different ways of writing $A=U\\Sigma V^T$.  You will need to pay careful attention when reading books and papers to see which one is being used.  On the other hand, it really does all just boil down to the fact that any rank-$r$ matrix can be written in the form\n",
    "\n",
    "$$\n",
    "    A = \\sum_{i=1}^r\\sigma_iu_iv_i^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPtgm3r9ewOL"
   },
   "source": [
    "# Geometric interpretation of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv4ZH6OpewOL"
   },
   "source": [
    "As with most concepts in linear algebra, the SVD has a geometric interpretation.  [This paper](http://www.ams.org/samplings/feature-column/fcarc-svd) gives a very nice overview of that interpretation, along with some applications.  Read the paper, and then continue with this notebook below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-4tyEk0ewOL"
   },
   "source": [
    "# Low-rank approximation and image compressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxsmSlFPewOL"
   },
   "source": [
    "As the paper suggests, if you're looking for a rank-$k$ approximation to a matrix $A$, a good choice is to use the SVD:\n",
    "\n",
    "$$\n",
    "    A\\approx A_k:=\\sum_{i=1}^k\\sigma_ku_iv_i^T.\n",
    "$$\n",
    "\n",
    "In fact, this is the *best* approximation in a very particular sense, but we don't need to worry about that right now.  The important idea for now is that low-rank approximations can be used to \"compress\" the data in $A$.  Of course, if we represent $A_k$ as a full $m\\times n$ matrix, we haven't compressed anything.  However, suppose we don't compute and store $A_k$, but instead we store the two matrices\n",
    "\n",
    "$$\n",
    "    X = [ u_1 \\, u_2 \\, \\cdots \\, u_k ]\\qquad\\text{and}\\qquad Y = [ \\sigma_1 v_1 \\quad \\sigma_2 v_2 \\quad \\cdots \\quad \\sigma_k v_k ].\n",
    "$$\n",
    "\n",
    "Using block matrix multiplication, you can verify that $A_k = XY^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-Cuye_qewOL"
   },
   "source": [
    "## 📝 Compression ratio\n",
    "\n",
    "When data is compressed, the *compression ratio*\n",
    "\n",
    "$$\n",
    "    \\text{compression ratio} = \\frac{\\text{uncompressed size}}{\\text{compressed size}}.\n",
    "$$\n",
    "\n",
    "1. What is the compression ratio attained by representing an $m\\times n$ matrix as $X$ and $Y$ as described above?  (Notice that units cancel in the ratio.  It might help to think of 1 number as a storage unit, so the uncompressed size is $mn$, since this is the quantity of numbers being stored in the $m\\times n$ matrix $A$.)\n",
    "2. What is the \"break even\" point for square matrices?  In other words, if $A$ is $n\\times n$, for what value of $k$ is the compression ratio exactly 1?  (For $k$ larger than this, we're doing the opposite of compression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "Oqx21zMLewOM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12abbcfc221f6ddcb3d173cba9131db0",
     "grade": true,
     "grade_id": "cell-2763015614c753a0",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRgwLNEvewOM"
   },
   "source": [
    "## 💻 Write your own low-rank approximation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFNUODcLewOM"
   },
   "source": [
    "Now, let's write some code.  Implement the function below as it is described in the docstring.  You'll want to use the [svd](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) function in numpy's linalg submodule.  Make sure your code passes the tests in the testing cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1674424670897,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "4kGSbx2aewOM",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f32001030c9fcb2d5242e44c1433511a",
     "grade": false,
     "grade_id": "cell-5186accc93cb4995",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def low_rank_approx(A,k):\n",
    "    \"\"\"\n",
    "    Computes a low-rank approximation of a matrix using the SVD.\n",
    "\n",
    "    Args:\n",
    "        A: a numpy array representing an m-by-n matrix\n",
    "        k: the desired rank of the approximation\n",
    "\n",
    "    Returns:\n",
    "        X, YT (numpy arrays): the low-rank approximation\n",
    "        \n",
    "        \n",
    "        \n",
    "    Given the rank-k approximation to A of the form\n",
    "\n",
    "        A_k = sigma_1 u_1 v_1^T + sigma_2 u_2 v_2^T + ... + sigma_k u_k v_k^T,\n",
    "\n",
    "    X is defined to be the m-by-k matrix with columns u_1, ... u_k.\n",
    "    \n",
    "    YT is defined to be the k-by-n matrix with rows sigma_1 v_1^T, ..., sigma_k v_k^T.\n",
    "    \n",
    "    The approximation can be formed by computing A_k = X.dot(YT).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from numpy.linalg import svd\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1674424671803,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "-S3W1qd4ewOO",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05e77a6101b043c20283fc9ec5ec50d5",
     "grade": true,
     "grade_id": "cell-3757a2f96a585ce7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "b0fa5b64-c803-499a-c4db-27c1d51785c7"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# test the low-rank approximation function\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import numpy.linalg as lna\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "# seed the random number generator\n",
    "rnd.seed(871980102)\n",
    "\n",
    "# dimensions and ranks of test cases\n",
    "tests = ((14,21,1),(9,4,2),(11,21,8),(18,9,7),(86,142,18),(118,118,91))\n",
    "\n",
    "# check cases where k is actually the rank\n",
    "for m,n,r in tests:\n",
    "    \n",
    "    # generate a random rank-r matrix\n",
    "    A = rnd.randn(m,r).dot( rnd.randn(r,n) )\n",
    "    \n",
    "    # get the low rank approximation\n",
    "    X,Y = low_rank_approx(A,r)\n",
    "    \n",
    "    # check the shapes of X and Y\n",
    "    assert( X.shape == (m,r) )\n",
    "    assert( Y.shape == (r,n) )\n",
    "    \n",
    "    # check that X is orthonormal \n",
    "    assert_almost_equal( lna.norm( X.T.dot(X) - np.eye(r) ), 0 )\n",
    "    \n",
    "    # check that the \"approximation\" is exact\"\n",
    "    assert_almost_equal( lna.norm(X.dot(Y) - A), 0 )\n",
    "    \n",
    "# use the fact that we know the 2-norm distance to the nearest rank-k approximation\n",
    "for m,n,r in tests:\n",
    "    \n",
    "    # generate a random full-rank matrix\n",
    "    A = rnd.randn(m,n)\n",
    "    \n",
    "    # get the low rank approximation\n",
    "    X,Y = low_rank_approx(A,r)\n",
    "    \n",
    "    # check the shapes of X and Y\n",
    "    assert( X.shape == (m,r) )\n",
    "    assert( Y.shape == (r,n) )\n",
    "    \n",
    "    # check that X is orthonormal \n",
    "    assert_almost_equal( lna.norm( X.T.dot(X) - np.eye(r) ), 0 )\n",
    "    \n",
    "    # get the singular values for A\n",
    "    S = lna.svd(A,compute_uv=False)\n",
    "    \n",
    "    # the 2-norm approximation error should be equal to the sigma_(r+1)\n",
    "    assert_almost_equal( lna.norm(X.dot(Y)-A,2)-S[r], 0 )\n",
    "    \n",
    "# if the code got here, everything worked fine\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyHOsf6bewOQ"
   },
   "source": [
    "## Image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx80odsWewOQ"
   },
   "source": [
    "Now that your low-rank approximation function is working, here's an application to image compression.  A standard representation of a color image on a computer is as three $m\\times n$ matrices, one for each of the RGB (red, green, blue) components.  These matrices will contain integer entries in the range 0 to 255.  The following code cell shows how this works.  You may want to read a little about [RGB](https://en.wikipedia.org/wiki/RGB_color_model) if you want to understand better what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 2132,
     "status": "ok",
     "timestamp": 1674424673569,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "Qz9sY4jiewOQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9a2bc13d-5cf9-4454-a1c4-08e9c51f8088"
   },
   "outputs": [],
   "source": [
    "# load some libraries we'll need\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "# define the RGB components of our test image\n",
    "red = np.array([\n",
    "    [255,   0, 255,   0,   0,   0,   0,   0,   0],\n",
    "    [255,   0, 255,   0,   0,   0,   0,   0,   0],\n",
    "    [255, 255, 255,   0,   0,   0,   0,   0,   0],        \n",
    "    [255,   0, 255,   0,   0,   0,   0,   0,   0],\n",
    "    [255,   0, 255,   0,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "    [255, 255, 255,   0,   0,  75, 238, 127, 255]\n",
    "],dtype='uint8')\n",
    "\n",
    "green = np.array([\n",
    "    [  0,   0,   0,   0, 255, 255, 255,   0,   0],\n",
    "    [  0,   0,   0,   0,   0, 255,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0, 255,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0, 255,   0,   0,   0],\n",
    "    [  0,   0,   0,   0, 255, 255, 255,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "    [  0, 127, 255, 255,   0,   0, 130, 127, 255]\n",
    "],dtype='uint8')\n",
    "\n",
    "blue = np.array([\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 255],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 255],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 255],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 255],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0, 255, 130, 238, 127, 255]\n",
    "],dtype='uint8')\n",
    "\n",
    "# combine to form the image\n",
    "img = np.dstack((red,green,blue))\n",
    "\n",
    "# describe the img array\n",
    "print('the img array has shape ', img.shape)\n",
    "\n",
    "# draw the image\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7A5zMYxewOR"
   },
   "source": [
    "This is a $7\\times 9$ pixel image.  The following function plots the image along with the magnitudes (shown in grayscale) of the RGB components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1674424673603,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "P53WIlUUewOR",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "c8011194-0013-4cea-e1aa-80b8b51e0b91"
   },
   "outputs": [],
   "source": [
    "def show_image_components(img,title='image'):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    comp_names = ('red','green','blue')\n",
    "    \n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    for comp in (0,1,2):\n",
    "        plt.subplot(1,4,comp+2)\n",
    "        plt.imshow(img[:,:,comp],cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(comp_names[comp])\n",
    "        \n",
    "show_image_components(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DSGdF7bewOR"
   },
   "source": [
    "## 📝 Trying it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa5SlQsZewOR"
   },
   "source": [
    "Here's a more interesting image.  (Make sure you've downloaded the two image files and put them in the same directory as your notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 4048,
     "status": "ok",
     "timestamp": 1674424676377,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "eF2Dn7fGewOR",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8479ce3d-ce7c-4b2b-fb05-f36f98f403dd"
   },
   "outputs": [],
   "source": [
    "img = io.imread(\"mondrian.png\")\n",
    "show_image_components(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrm5MTwbewOS"
   },
   "source": [
    "Now let's try some compression.  The idea is pretty simple.  Instead of storing each of the three $m\\times n$ matrices for the three RGB components, we can instead store the corresponding $X$ and $Y^T$ matrices that come from your low-rank approximation above.  The following function shows how that would work.\n",
    "\n",
    "**Note:** Our earlier calculation of the 'compression ratio' was a little sloppy due to the fact that the original image data is actually stored as an array of bytes (8 bits per entry) while the $X$ and $Y^T$ arrays are stored as floating point numbers (64 bits per entry).  The function given computes a similar quantity (the percent decrease in storage size) more accurately.  In the real world, we'd want to give more thought to exactly how we should store these numbers.  In practice, though, the SVD is not a particularly strong image compression technique, so there's not a lot of point in working on optimizing storage right now.  Think of this as more a conceptual exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 2809,
     "status": "ok",
     "timestamp": 1674424677977,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "oBPA9KRjewOS",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2b02d06d-5dd7-4fbb-885f-6607103769d1"
   },
   "outputs": [],
   "source": [
    "def compress_image(img,k):\n",
    "    \"\"\"\n",
    "    Performs SVD-based image compression and compares the compressed image\n",
    "    to the original.\n",
    "\n",
    "    Args:\n",
    "        img: an RGB image array (m-by-n-by-3)\n",
    "        k:   the approximation rank\n",
    "\n",
    "    Returns:\n",
    "        Nothing.  This is just a demo function.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    # compute the low-rank approximations of the RGB components\n",
    "    # (These are the 6 arrays we would store to compress the image.)\n",
    "    Xr,YTr = low_rank_approx(img[:,:,0],k)\n",
    "    Xg,YTg = low_rank_approx(img[:,:,1],k)\n",
    "    Xb,YTb = low_rank_approx(img[:,:,2],k)\n",
    "    \n",
    "    # expand the compressed image for comparison with the original\n",
    "    imgc = np.dstack((Xr.dot(YTr),Xg.dot(YTg),Xb.dot(YTb)))\n",
    "\n",
    "    # for our image format, we need to represent each pixel as a triple of unsigned bytes\n",
    "    # so here we take a few steps to conver the floating point array into a byte array\n",
    "    imgc[imgc<0]   = 0\n",
    "    imgc[imgc>255] = 255\n",
    "    imgc = imgc.astype('uint8')\n",
    "\n",
    "    # show the original and reconstructed image for comparison\n",
    "    show_image_components(img,'original')\n",
    "    show_image_components(imgc,'compressed')\n",
    "    \n",
    "    \n",
    "    # compare the compressed and original storage sizes\n",
    "    new_size  = 3*(Xr.size+YTr.size)*8\n",
    "    orig_size = img.size\n",
    "\n",
    "    print('Compressed using {} singular values, a(n) {:.3f} % decrease in size'.format( k, 100.*(orig_size-new_size)/orig_size ))\n",
    "\n",
    "img = io.imread(\"mondrian.png\")\n",
    "compress_image(img,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIiP9ZmqewOS"
   },
   "source": [
    "\n",
    "Experiment with the approximation rank until you find the lowest rank that gives what you think is a reasonable representation of the original image.  What rank looks good to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "Wnp_4zoLewOS",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e92353d7c2c51e89070439409f650f9",
     "grade": true,
     "grade_id": "cell-962d8a35a88cd02c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2yMRU6JewOS"
   },
   "source": [
    "## 📝 Something to think about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezaIG8crewOT"
   },
   "source": [
    "The following shows the same process for a rotate version of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1674424678024,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "pc7mP174ewOT",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5b1d8257-f51b-40a1-a1ff-1bce2e1a0bba"
   },
   "outputs": [],
   "source": [
    "img = io.imread(\"mondrian_r.png\")\n",
    "compress_image(img,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWHLPD4LewOT"
   },
   "source": [
    "Experiment with the parameter $k$ in the code above.  How does it compare to the unrotated example?  Why do you think the two examples behave differently?  What types of images will SVD-based compression work best for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "b4nKDkM5ewOT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b151138c0bb36d53f879a08a6594b8a8",
     "grade": true,
     "grade_id": "cell-bbae14b6feef79a7",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1674424678071,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "I-dzCbu8ewOT",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "math726",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbgrader": {
   "cocalc_minimal_stubs": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b2d0c14b283a7f9e610fd695b30c77a0ab4bd3698df597db9ae22fb81b71cbe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
