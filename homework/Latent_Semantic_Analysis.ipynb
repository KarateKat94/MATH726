{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG-nsSMIo4e_"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejIMIB1Ro4fC"
   },
   "source": [
    "In this notebook, we'll look at one application of the SVD: Latent Semantic Analysis (LSA).  Although this example is a very small and simplified implementation, it should provide you with a flavor of how the SVD can be used to expose low-dimensional structure in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu4kSy0eo4fC"
   },
   "source": [
    "# From documents to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgfZzNQIo4fC"
   },
   "source": [
    "Using some vocabulary from the field, we'll be working with a collection (or *corpus*) of *documents* (or *texts*).  For our purposes, each document is just a string of English words and the corpus is an ordered list of these documents.  These strings come from the paper [here](http://lsa.colorado.edu/papers/dp1.LSAintro.pdf).  We'll be following the example introduced at the beginning of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674930540561,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "uR6fHjgHo4fD",
    "outputId": "707818dd-814f-4426-e645-89c51c373854"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Human machine interface for ABC computer applications',\n",
    "    'A survey of user opinion of computer system response time',\n",
    "    'The EPS user interface management system',\n",
    "    'System and human system engineering testing of EPS',\n",
    "    'Relation of user perceived response time to error measurement',\n",
    "    'The generation of random, binary, ordered trees',\n",
    "    'The intersection graph of paths in trees',\n",
    "    'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    "    'Graph minors: A survey'\n",
    "]\n",
    "\n",
    "print('There are {} documents in the corpus.'.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG9VyepDo4fE"
   },
   "source": [
    "The next step is to chop the documents into meaningful *tokens*.  In this case, we'll break each document into a list of its words.  The following function takes care of this.  It also gets rid of all punctuation and converts every word to lowercase to allow for easier comparison among documents.  In more advanced implementations, we would also [*stem*](https://en.wikipedia.org/wiki/Stemming) the words so that, for example, the words 'ordered', 'order', 'ordering', etc. would all be treated as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1674930540775,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "mxU4URrCo4fE",
    "outputId": "476b67d4-7491-4834-fa8d-9c09a9cd6ec6"
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Tokenizes a document.\n",
    "\n",
    "    Args:\n",
    "        document: A string containing the document text.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens from the document.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    import string\n",
    "    \n",
    "    # remove punctuation\n",
    "    document = document.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    # convert all words to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # convert document to a list of words\n",
    "    tokens = document.split(' ')\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print('Tokens for each document')\n",
    "print()\n",
    "for document in documents:\n",
    "    print(document)\n",
    "    print(tokenize(document))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH9F1XCdo4fF"
   },
   "source": [
    "Now that we've tokenized each document, we need some way to embed them into a vector space.  There are many ways to do this, but one of the simplest is to assign each unique word its own axis, then each document is assigned a vector with coordinates given by the number of times each word appears in the document.  In practice, it's useful to exclude certain words.  For example, the word 'the' tends to carry very little semantic content.  In some applications, the set of *index terms* are chosen automatically by filtering a set of *stop words* out of the set of all words in the corpus.  For this example, we'll explicitly select a set of terms to use for vectorizing the documents.  (These are exactly the terms chosen on page 10 of the paper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1674930540775,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "8VsVyzM9o4fF",
    "outputId": "ea14e7fe-92ab-4da2-c211-b5ba5017bb32"
   },
   "outputs": [],
   "source": [
    "terms = [ 'human', 'interface', 'computer', 'user', 'system', 'response',\n",
    "          'time', 'eps', 'survey', 'trees', 'graph', 'minors' ]\n",
    "\n",
    "print('The documents will be embedded in {}-dimensional space.'.format(len(terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImOu6jXuo4fG"
   },
   "source": [
    "One last step before we can vectorize the documents.  We'll need a quick way to check if a word appears in the terms list and, if so, to determine where in the list it occurs.  In Python, this can be efficiently accomplished by creating a dictionary object that maps each term to its index, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674930540776,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "R64UlGNVo4fG",
    "outputId": "8a5091af-3b5f-4c68-ad8d-3902fd08f028"
   },
   "outputs": [],
   "source": [
    "term_id = { term:i for i,term in enumerate(terms) }\n",
    "\n",
    "for t in ('human', 'hamburger', 'salad', 'survey'):\n",
    "    if t in term_id:\n",
    "        print('{} is entry {} in the terms list'.format(t,term_id[t]))\n",
    "    else:\n",
    "        print('{} is not a term in the list'.format(t))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wro3Fc6ko4fG"
   },
   "source": [
    "Now we can create the vectorizer.  For each document, it should assign a vector $v$ with the property that $v_i$ is the number of times term $i$ appears in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wro3Fc6ko4fG"
   },
   "source": [
    "## ðŸ’» Exercise\n",
    "\n",
    "Implement the vectorize function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1674930541007,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "0g5uBcvBo4fG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d4acfcc0e3c0661cdfb5d60351afb59",
     "grade": false,
     "grade_id": "cell-df4f0d21a30b8a9b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(document,terms,term_id):\n",
    "    \"\"\"\n",
    "    Converts a document string into a vector.\n",
    "\n",
    "    Args:\n",
    "        document : The document string.\n",
    "        terms    : The list of index terms.\n",
    "        term_id  : Dictionary mapping each term to its index in 'terms'\n",
    "\n",
    "    Returns:\n",
    "        v : A numpy array with the property that v[i] contains the number\n",
    "            of times terms[i] appears in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    # intialize a zero vector\n",
    "    v = np.zeros(len(terms))\n",
    "    \n",
    "    # tokenize the document\n",
    "    tokens = tokenize(document)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1674930541007,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "YllqKgdco4fH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7325af42bd357b903341aad403810ecf",
     "grade": true,
     "grade_id": "cell-720be6c2f93436e9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "96c654c4-17bd-4433-b4c7-960ab8db567c"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# small batch of multi-word tests\n",
    "#\n",
    "import numpy as np\n",
    "from numpy.testing import assert_equal\n",
    "\n",
    "test_documents = [ 'no terms here', 'human human crazy human', 'survey graph survey monkey survey' ]\n",
    "test_answers   = [\n",
    "    np.array( [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.] ),\n",
    "    np.array( [3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.] ),\n",
    "    np.array( [0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 0.] )\n",
    "]\n",
    "for i in range(len(test_documents)):\n",
    "    assert_equal( vectorize(test_documents[i],terms,term_id), test_answers[i] )\n",
    "    \n",
    "#\n",
    "# test that all one-word documents get the right vector\n",
    "#\n",
    "for i,t in enumerate(terms):\n",
    "    true    = np.zeros(len(terms))\n",
    "    true[i] = 1.\n",
    "    assert_equal( vectorize(terms[i],terms,term_id), true )\n",
    "    \n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juIb_naXo4fH"
   },
   "source": [
    "# The occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dP9eM-4o4fH"
   },
   "source": [
    "Suppose $m$ is the number of terms and $n$ is the number of documents.  The occurrence matrix $X$ is the $m\\times n$ matrix with $X_{ij}$ equal to the number of times term $i$ shows up in document $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dP9eM-4o4fH"
   },
   "source": [
    "## ðŸ’» Exercise\n",
    "\n",
    "Use your <tt>vectorize</tt> function to implement the following function to build the occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674930541007,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "8tu6CEBKo4fH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18d811f2e287b9d3c373f5ead8000c58",
     "grade": false,
     "grade_id": "cell-00334822dfac2494",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def occurrence_matrix(documents,terms,term_id):\n",
    "    \"\"\"\n",
    "    Forms the occurrence matrix for a corpus of documents.\n",
    "\n",
    "    Args:\n",
    "        documents : Iterable of document strings.\n",
    "        terms     : The list of index terms.\n",
    "        term_id   : Dictionary mapping each term to its index in 'terms'\n",
    "\n",
    "    Returns:\n",
    "        X : A term-by-document numpy array where each column is the occurrence vector of\n",
    "            the corresponding document.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # initialize empty matrix\n",
    "    m, n = len(terms), len(documents)\n",
    "    X = np.zeros((m,n))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674930541007,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "hzdc8Mhzo4fH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51c36b675c539401d95b8d5793ba3f40",
     "grade": true,
     "grade_id": "cell-ac2aeeea0ad07c5e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "e9a57752-45ea-44c6-9942-3845b8fb2b59"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# small test\n",
    "#\n",
    "test_documents = ['a b c', 'a c d', 'c d f c']\n",
    "test_terms = [ 'a', 'b', 'c', 'd' ]\n",
    "test_term_id = { test_terms[i] : i for i in range(len(test_terms)) }\n",
    "X_true = np.array([[1., 1., 0.], [1., 0., 0.], [1., 1., 2.], [0., 1., 1.]])\n",
    "X = occurrence_matrix(test_documents,test_terms,test_term_id)\n",
    "assert_equal(X,X_true)\n",
    "\n",
    "#\n",
    "# make sure the code is using vectorize (i.e., not a copy/paste job)\n",
    "#\n",
    "zz_vectorize = vectorize\n",
    "del vectorize\n",
    "try:\n",
    "    occurrence_matrix(documents,terms,term_id)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError('occurrence_matrix does not use vectorize')\n",
    "finally:\n",
    "    vectorize = zz_vectorize\n",
    "    del zz_vectorize\n",
    "    \n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcEcHzg2o4fI"
   },
   "source": [
    "Here's a handy function that will print out the occurrence matrix.  The table it prints should match the one in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674930541007,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "OCgAoLROo4fI",
    "outputId": "136f5cc2-b589-481a-a926-ac4d1a396802"
   },
   "outputs": [],
   "source": [
    "def print_occurrence_matrix(X,terms):\n",
    "    m,n = X.shape\n",
    "    print(' '*10, end='')\n",
    "    for j in range(n):\n",
    "        print('{:5} '.format(j), end='')\n",
    "    print()\n",
    "    for i in range(m):\n",
    "        print('{:10}'.format(terms[i]), end='')\n",
    "        for j in range(n):\n",
    "            print('{: .2f} '.format(X[i,j]), end='')\n",
    "        print()\n",
    "\n",
    "X = occurrence_matrix( documents, terms, term_id )\n",
    "print_occurrence_matrix(X,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn-97UIPo4fI"
   },
   "source": [
    "# Computing similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtY48tRco4fI"
   },
   "source": [
    "The occurrence matrix provides us a vector representing each document (the columns of $X$), as well as a vector representing each word (the rows of $X$).  For example, document 0 in the corpus is represented as the vector\n",
    "\n",
    "$$ [1\\; 1\\; 1\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0 ]^T $$\n",
    "\n",
    "while the term 'human' is represented as the vector\n",
    "\n",
    "$$ [ 1\\; 0\\; 0\\; 1\\; 0\\; 0\\; 0\\; 0\\; 0 ]^T. $$\n",
    "\n",
    "One useful thing we might want to be able to do is to compare two documents or two terms for similarity.  A common technique is to use a measure called *cosine similarity*, which is based on the cosine of the angle between the two vector representations.  Given two vectors $x$ and $y$, the cosine similarity between $x$ and $y$ is computed as\n",
    "\n",
    "$$\n",
    "    \\text{similarity}(x,y) = \\frac{x^Ty}{\\|x\\|\\,\\|y\\|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtY48tRco4fI"
   },
   "source": [
    "## ðŸ’» Exercise\n",
    "\n",
    "Implement the cosine similarity function below.  Make sure it passes all tests before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1674930541008,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "13ivJOAAo4fI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0a085c66dad31aa41eb4ece32e86bec",
     "grade": false,
     "grade_id": "cell-eb90075bc64a8577",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        x: First vector as a numpy array.\n",
    "        y: Second vector as a numpy array.\n",
    "\n",
    "    Returns:\n",
    "        The consine similarity between x and y.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1674930541008,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "1AxsorHJo4fI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be1ba3c56da39d797219c208898d3102",
     "grade": true,
     "grade_id": "cell-b082694dc89ef207",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "21df9374-c413-48e1-b70a-dcaf95417f19"
   },
   "outputs": [],
   "source": [
    "# tests for cosine similarity\n",
    "from numpy.testing import assert_almost_equal\n",
    "import numpy as np\n",
    "\n",
    "tests = (([1,0],[0,1],0),([1,0,0],[3,0,0],1),([1,1,1,1],[1,1,1,0],np.sqrt(3)/2,),\n",
    "         ([-1,2,3,-4],[0,-1,0,5],-11./np.sqrt(195)))\n",
    "\n",
    "for x,y,sim in tests:\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    assert_almost_equal(cosine_similarity(x,y),sim)\n",
    "    \n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMkWqY-do4fI"
   },
   "source": [
    "The following code cell shows a way of visualizing the pairwise similarity among our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1674930541500,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "vSh6o7m2o4fJ",
    "outputId": "714c2eaf-8260-4593-d965-d142b70c4600"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "m,n = X.shape\n",
    "sim = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(i,n):\n",
    "        sim[i,j] = cosine_similarity(X[:,i],X[:,j])\n",
    "        sim[j,i] = sim[i,j]\n",
    "        \n",
    "plt.imshow(sim,interpolation='nearest',cmap='viridis')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtBBnW7co4fJ"
   },
   "source": [
    "The colors of the square pixels in the image show the values of the cosine similarity among the documents.  The ones on the diagonal show the obvious observation that every vector is parallel to itself.  The block-diagonal structure of the matrix suggests that the documents have already been ordered into two groups of similar documents.  The following cell mixes up the order of the documents to give a more realistic picture.  (Just so you don't get the impression that a typical document-document similarity plot will have a nice block-diagonal structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1674930541910,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "8WWoVG2Oo4fJ",
    "outputId": "094828b5-5d00-42f4-f546-fa169024c928"
   },
   "outputs": [],
   "source": [
    "m,n = X.shape\n",
    "np.random.seed(7182001)\n",
    "Z = X[:,np.random.permutation(n)]\n",
    "sim = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(i,n):\n",
    "        sim[i,j] = cosine_similarity(Z[:,i],Z[:,j])\n",
    "        sim[j,i] = sim[i,j]\n",
    "        \n",
    "plt.imshow(sim,interpolation='nearest',cmap='viridis')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6XgK7zqo4fJ"
   },
   "source": [
    "The following cell shows the similarity among terms.  You'll notice again a strong suggestion that the authors of the paper have already arranged the terms into two groups for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1674930542319,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "BSOMiQuPo4fJ",
    "outputId": "097df19b-540c-4177-c9a6-210fbd360235"
   },
   "outputs": [],
   "source": [
    "m,n = X.shape\n",
    "sim = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    for j in range(i,m):\n",
    "        sim[i,j] = cosine_similarity(X[i,:],X[j,:])\n",
    "        sim[j,i] = sim[i,j]\n",
    "        \n",
    "plt.imshow(sim,interpolation='nearest',cmap='viridis')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWmRg5wko4fJ"
   },
   "source": [
    "Inspired by the paper, let's take a look at the similarity between term 0 ('human') and term 3 ('user').  Notice that the similarity value is 0 and, indeed, by inspecting $X$ we can verify that the two term vectors are orthogonal.  In a sense, this is hardly surprising.  We have a corpus of 9 documents vectorized by 12 terms.  The fact that there is no obvious connection between 'human' and 'user' can easily be attributed to a lack of sufficient information.  However, rather than just leave off with this explanation, the authors make an interesting claim: that the SVD can uncover a connection between the terms.  Although it can be a little tricky to formalize the argument in a purely mathematical context, the basic idea appears to be related to the \"curse of dimensionality\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-WeLr8do4fJ"
   },
   "source": [
    "# The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jqjh0PZo4fJ"
   },
   "source": [
    "The \"curse of dimensionality\" is a phrase that is used by people working in big data to signify a fundamental difficulty in the field: high-dimensional spaces are weird.  By weird, we mean that things that seem intuitive in our usual 3-dimensional world are not so obvious and sometimes even *wrong* in higher dimensions.  A bunch of examples can be found in [this paper](https://www.math.ucdavis.edu/~strohmer/courses/180BigData/180lecture1.pdf), including the \"orange peel\" problem.  In 3D, when you remove a thin shell (the orange peel) from a sphere (the orange), you still have a lot of volume left to... eat.  For a high-dimensional orange, though, once you remove the peel, there's almost nothing left.  In more mathematical terms, the volume of a high-dimensional sphere is concentrated near its boundary.\n",
    "\n",
    "Another example of weirdness has to do with the angle between random directions.  Suppose we pick two directions in $d$-dimensional space at random.  What would you expect the angle between these two directions to be?  As the following simulation shows, the answer depends on $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 3433,
     "status": "ok",
     "timestamp": 1674930545747,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "GbiJm4Noo4fJ",
    "outputId": "61549703-cfe8-4c2d-fe38-557a6abd8fce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of direction pairs to choose\n",
    "n_samples = 10000\n",
    "\n",
    "# seed the random number generator so we all see the same thing\n",
    "np.random.seed(18477718)\n",
    "\n",
    "# places to store the samples\n",
    "samples_low_d = np.zeros(n_samples)\n",
    "samples_high_d = np.zeros(n_samples)\n",
    "\n",
    "# loop to collect samples\n",
    "for i in range(n_samples):\n",
    "    \n",
    "    # two random vectors in low-D\n",
    "    x = np.random.randn(3)\n",
    "    y = np.random.randn(3)\n",
    "    samples_low_d[i] = np.arccos(cosine_similarity(x,y))\n",
    "\n",
    "    # two random vectors in high-D\n",
    "    x = np.random.randn(1000)\n",
    "    y = np.random.randn(1000)\n",
    "    samples_high_d[i] = np.arccos(cosine_similarity(x,y))\n",
    "\n",
    "# plot the cosine histograms\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(samples_low_d,bins=50,range=[0,np.pi]);\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('3D vectors')\n",
    "plt.xlim([0,np.pi])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(samples_high_d,bins=50,range=[0,np.pi])\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('1000-D vectors')\n",
    "plt.xlim([0,np.pi]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOvSsmX1o4fK"
   },
   "source": [
    "If you pick two random directions 3D, the distribution of angles is fairly spread out.  The mean appears to be $\\pi/2$, but nearly-parallel directions are quite possible.  In high dimensions, though, the two directions are almost certainly going to be nearly perpendicular.  This is a problem for the following reason.\n",
    "\n",
    "We often analyze a dataset in order to infer relationships among variables.  When we do this, we are essentially hoping that our observations can be explained using only a fairly small number of variables.  However, when we're sampling the data \"in the wild,\" we don't know which variables these are.  So, we just collect everything we can get our hands on.  The problem with this approach is that, by sampling irrelevant data, we are polluting our dataset with a bunch of random noise.  Imagine, for example, taking a nice 2D dataset where the relationships among variables is clear and then embedding this data into 100D and adding random noise in the 98 irrelevant directions.  Suddenly, vectors that lined up perfectly in the 2D data are almost orthogonal in 100D.  In a sense, there's simply way too much \"room\" in high-dimensional spaces, making it difficult to analyze data in them.  So a huge part of analyzing big data is to reduce the number of dimensions (hopefully by keeping important dimensional data while discarding unimportant data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTKKWvJ0o4fK"
   },
   "source": [
    "# Dimension reduction and the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbfP5Gpio4fK"
   },
   "source": [
    "One of the simplest (and most widely used) techniques for dimension reduction is linear projection.  Given data represented as vectors in $d$-dimensional space, we choose a $k$-dimensional subspace, project our data there, and then analyze the lower-dimensional result.  But how do we pick the subspace, or $k$ for that matter?  In general, these aren't easy questions.  There is a lot of trial-and-error involved, and each discipline tends to develop its own conventions by repeated testing and tweaking over time.\n",
    "\n",
    "The technique used in the LSA paper is to use the SVD to pick out the important dimensions to use.  Without getting too deep into the theory (and speculation), here's the basic idea: pick a $k$ (The authors use $k=2$ for the tiny data set we've been working with; in real applications you should imagine $k$ in the hundreds or maybe thousands.) and replace the occurrence matrix $X$ by its best rank-k approximation $X_k$.  The following cells give a very simple implementation of this idea followed by a test to make sure the code reproduces the $X_k$ from the paper.  (In the real world, we would likely store the vectors $U_k$ and $\\Sigma_k V_k^T$ rather than compute $X_k$ explicitly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674930545747,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "0jC1dLVpo4fK"
   },
   "outputs": [],
   "source": [
    "def low_rank_approx(X,k):\n",
    "    \"\"\"\n",
    "    Produces a low-rank approximation of a matrix.\n",
    "\n",
    "    Args:\n",
    "        X: The matrix to approximate (a numpy array).\n",
    "        k: The target rank.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array holding the best rank-k approximation to X.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    from numpy.linalg import svd\n",
    "\n",
    "    # compute the SVD\n",
    "    U,S,VT = svd(X)\n",
    "    \n",
    "    # restrict to rank k\n",
    "    U = U[:,:k]\n",
    "    S = S[:k]\n",
    "    VT = VT[:k,:]\n",
    "    \n",
    "    return U.dot(np.diag(S)).dot(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674930545747,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "r4DiOYrJo4fK",
    "outputId": "bc4c47bf-7c5f-4f9a-acbd-4ce3c2c3ce01"
   },
   "outputs": [],
   "source": [
    "Xk = low_rank_approx(X,2)\n",
    "\n",
    "# here is the reduced matrix given in the paper\n",
    "# (it only has two digits of accuracy)\n",
    "Xk_true = np.array( [\n",
    "    [ 0.16, 0.40, 0.38, 0.47, 0.18, -0.05, -0.12, -0.16, -0.09 ],\n",
    "    [ 0.14, 0.37, 0.33, 0.40, 0.16, -0.03, -0.07, -0.10, -0.04 ],\n",
    "    [ 0.15, 0.51, 0.36, 0.41, 0.24, 0.02, 0.06, 0.09, 0.12 ],\n",
    "    [ 0.26, 0.84, 0.61, 0.70, 0.39, 0.03, 0.08, 0.12, 0.19 ],\n",
    "    [ 0.45, 1.23, 1.05, 1.27, 0.56, -0.07, -0.15, -0.21, -0.05 ],\n",
    "    [ 0.16, 0.58, 0.38, 0.42, 0.28, 0.06, 0.13, 0.19, 0.22 ],\n",
    "    [ 0.16, 0.58, 0.38, 0.42, 0.28, 0.06, 0.13, 0.19, 0.22 ],\n",
    "    [ 0.22, 0.55, 0.51, 0.63, 0.24, -0.07, -0.14, -0.20, -0.11 ],\n",
    "    [ 0.10, 0.53, 0.23, 0.21, 0.27, 0.14, 0.31, 0.44, 0.42 ],\n",
    "    [ -0.06, 0.23, -0.14, -0.27, 0.14, 0.24, 0.55, 0.77, 0.66 ],\n",
    "    [ -0.06, 0.34, -0.15, -0.30, 0.20, 0.31, 0.69, 0.98, 0.85 ],\n",
    "    [ -0.04, 0.25, -0.10, -0.21, 0.15, 0.22, 0.50, 0.71, 0.62 ] ] )\n",
    "\n",
    "# check the relative error\n",
    "assert( np.linalg.norm(Xk-Xk_true)/np.linalg.norm(Xk_true) < 1e-2 )\n",
    "\n",
    "print('The X_k matrix is correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arQT2pQxo4fK"
   },
   "source": [
    "The following table shows the approximate occurrence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1674930545962,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "6SvbQRVuo4fK",
    "outputId": "801ce661-25a9-4d7e-8914-0fc7c52fe1a2"
   },
   "outputs": [],
   "source": [
    "print_occurrence_matrix(Xk,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mscjiEMto4fK"
   },
   "source": [
    "Note that it's trickier to understand exactly what these numbers mean than it was before.  What do fractional or negative occurrences mean?  Over time, people working with LSA developed some intuitive ways of interpreting these numbers.  (Take a look at p. 12 of the paper, for example.)  Although it's interesting, we don't really need to develop the same intuition in order to appreciate what happens next.  Compare the following term-term similarity matrices.  The first is the one we saw before.  The second is the result of using the SVD to project the data into 2 dimensions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 1893,
     "status": "ok",
     "timestamp": 1674930547852,
     "user": {
      "displayName": "Nathan Albin",
      "userId": "12921257652380671254"
     },
     "user_tz": 360
    },
    "id": "MLcE6m5So4fL",
    "outputId": "9ffc08f9-23aa-491f-b8ff-3c54a0fc7067"
   },
   "outputs": [],
   "source": [
    "m,n = X.shape\n",
    "sim = np.zeros((m,m))\n",
    "sim_k = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    for j in range(i,m):\n",
    "        sim[i,j] = cosine_similarity(X[i,:],X[j,:])\n",
    "        sim[j,i] = sim[i,j]\n",
    "        sim_k[i,j] = cosine_similarity(Xk[i,:],Xk[j,:])\n",
    "        sim_k[j,i] = sim_k[i,j]\n",
    "        \n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sim,interpolation='nearest',cmap='viridis')\n",
    "plt.title('original')\n",
    "plt.colorbar();\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sim_k,interpolation='nearest',cmap='viridis')\n",
    "plt.title('low-rank approx')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2pTZtmFo4fL"
   },
   "source": [
    "# Learning more about LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQjLG194o4fL"
   },
   "source": [
    "The authors of the LSA paper suggest that the figure on the right above demonstrates the power of using the SVD for semantic analysis.  By throwing out some irrelevant dimensions, we are able to detect a relationship between the terms 'human' and 'user' within the corpus.  Whether or not this is true is somewhat academic; it's nearly impossible to make an inference from a single observation.  Nonetheless, it does give us something to think about.  Moreover, numerous tests of this technique (and some significant refinements) on much larger corpora have generated a body of evidence that suggest that SVD-based LSA can be useful in certain applications.  If you're interested, you can read more of the paper and/or search the web for 'latent semantic analysis'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omcvy3xko4fL"
   },
   "source": [
    "# Learning more about projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt5bL-Rmo4fL"
   },
   "source": [
    "The theory of projections and its relationship to the SVD is a rich subject that certainly can't be covered in one Jupyter notebook.  Here's a taste of some of that theory that will help you with the quiz for this module.  First, a quick review of some linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pzrU-EDo4fL"
   },
   "source": [
    "## Four fundamental subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNxHyPQ9o4fL"
   },
   "source": [
    "Given a matrix $A\\in\\mathbb{R}^{m\\times n}$, the *range* of $A$, $R(A)$, and *nullspace* of $A$, $N(A)$, are defined as\n",
    "\n",
    "$$\n",
    "    R(A) = \\{Ax : x\\in\\mathbb{R}^m\\}\\qquad\\text{and}\\qquad\n",
    "    N(A) = \\{x\\in\\mathbb{R}^n : Ax = 0 \\}.\n",
    "$$\n",
    "\n",
    "**Note:** As with many things in linear algebra, there is no universally accepted convention for the notation of range and nullspace.  For example, some authors will denote the range of $A$ as $\\text{rng}(A)$ or $\\text{ran}(A)$.  Others will call it the *column space* and denote it $\\text{col}(A)$.  The nullspace might be denoted $\\text{nul}(A)$ or $\\text{null}(A)$.  Some authors call it the *kernel* and denote it $\\text{ker}(A)$.  It's important to be aware of these differences and to make sure you know what concept is being talked about when you switch between papers and books.\n",
    "\n",
    "A standard exercise in undergrad linear algebra is to verify that $R(A)$ is a subspace of $\\mathbb{R}^n$ and $N(A)$ is a subspace of $\\mathbb{R}^m$.  Given any subspace $S$ of a vector space, $\\mathbb{R}^n$ say, the *orthogonal subspace* $S^\\perp$ (often called \"S perp\") is defined as\n",
    "\n",
    "$$\n",
    "    S^\\perp = \\{ x\\in\\mathbb{R}^n : x^Ty = 0\\;\\forall y\\in S\\}.\n",
    "$$\n",
    "\n",
    "Again, verifying that $S^\\perp$ is a subspace is a standard exercise.  Typically, you would also see the fact that $\\mathbb{R}^n=S\\oplus S^\\perp$.  (If you need to find this in a book to review, the term you are looking for is \"direct sum.\")\n",
    "\n",
    "A modern view of linear algebra that has emerged is that of the \"Four fundamental subspaces\" of a matrix.  Here is a nice set of [lecture notes](http://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf) on the topic.  It's a beautiful idea that can be explained through the SVD as follows.\n",
    "\n",
    "Recall that if $A$ has rank $r$, then we can write the SVD of $A$ in a few different ways.  At the two extremes are the most compact form and the full form:\n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^T = [U_r\\;\\tilde{U}]\n",
    "    \\begin{bmatrix}\n",
    "    \\Sigma_r & 0 \\\\ 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    [V_r\\;\\tilde{V}]^T\n",
    "    = U_r\\Sigma_r V_r^T.\n",
    "$$\n",
    "\n",
    "As explained in the notes, the columns of the matrices $U_r$, $\\tilde{U}$, $V_r$ and $\\tilde{V}$ all span a different important subspace related to the matrices $A$ and $A^T$:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "    R(A)       = \\text{span } U_r = N(A^T)^\\perp\\\\\n",
    "    R(A)^\\perp = \\text{span } \\tilde{U} = N(A^T)\\\\\n",
    "    N(A)       = \\text{span } \\tilde{V} = R(A^T)^\\perp\\\\\n",
    "    N(A)^\\perp = \\text{span } V_r = R(A^T).\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zcqdKF9o4fL"
   },
   "source": [
    "## Orthogonal projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19jTXgKko4fL"
   },
   "source": [
    "Given a subspace $S$ of $\\mathbb{R}^n$ and a vector $x\\in\\mathbb{R}^n$, the goal of orthogonal projection can be expressed in optimization form as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{minimize}&\\quad \\|y-x\\|\\\\\n",
    "    \\text{subject to}&\\quad y\\in S.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In words, we want to find the vector $y$ in $S$ that is closest (in the 2-norm sense) to the given vector $x$.  If we have an orthonormal basis for $S$, there is a very simple formula for finding the projection.  Suppose $S$ has dimension $k$ and let $U\\in\\mathbb{R}^{n\\times k}$ have columns given by an orthonormal basis for $S$. (In other words, $S=\\text{span }U$ and $U^TU=I_{k\\times k}$.)  Then the orthogonal projection of $x$ onto $S$ is given by $y=UU^Tx$.  The matrix $P=UU^T$ is called the projection matrix.  Watch the following video to get a better understanding of how projections work.\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://youtu.be/1Mqq2rLV910\">\n",
    "                <img src=\"https://img.youtube.com/vi/1Mqq2rLV910/hqdefault.jpg\"><br>\n",
    "                Projections and Orthogonal Matrices\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "\n",
    "Looking back at the relationship between the SVD and the four fundamental subspaces, we can find some useful formulas:\n",
    "- the matrix $U_rU_r^T$ is the orthogonal projection onto $R(A)$,\n",
    "- the matrix $\\tilde{V}\\tilde{V}^T$ is the orthogonal projection onto $N(A)$,\n",
    "- and so on.\n",
    "\n",
    "Moreover, if $P$ is the orthogonal projection matrix onto the subspace $S$, then $I-P$ is the orthogonal projection matrix onto $S^\\perp$.  So another way to write the projection onto $N(A)$ is $I-V_rV_r^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47v-U5jXo4fL"
   },
   "source": [
    "## The Moore-Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouQP30DEo4fL"
   },
   "source": [
    "While we're on the topic of projections, it's also a good idea to recall the pseudoinverse operator.  Given a matrix $A\\in\\mathbb{R}^{m\\times n}$, the pseudoinverse is the operator $A^+\\in\\mathbb{R}^{n\\times m}$ with the following two properties.\n",
    "\n",
    "1. For any $b\\in\\mathbb{R}^m$, the vector $A^+b$ minimizes $\\|Ax-b\\|_2$ over all $x\\in\\mathbb{R}^n$.\n",
    "2. Among all the $x\\in\\mathbb{R}^n$ that minimize $\\|Ax-b\\|_2$, $A^+b$ is the one with smallest 2-norm.\n",
    "\n",
    "There are many useful ways of viewing the pseudoinverse.  One way, that relies heavily on projection matrices, is described in the following video.\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://youtu.be/RsX9ILHGAaQ\">\n",
    "                <img src=\"https://img.youtube.com/vi/RsX9ILHGAaQ/hqdefault.jpg\"><br>\n",
    "                SVD, Projection and the Pseudoinverse\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbgrader": {
   "cocalc_minimal_stubs": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b2d0c14b283a7f9e610fd695b30c77a0ab4bd3698df597db9ae22fb81b71cbe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
